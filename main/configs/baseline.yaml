dataset_name: "NerveDataset"
model_name: "JointModel"
model_parameters: {
  "in_channels": 1,
  "out_channels": 1,
  "img_size_w": 320,
  "img_size_h": 336,
  "patch_size": 16,
  "decoder_dim": 768,
  "masking_ratio": 0.75
}
training_parameters: {
  "learning_rate": 0.0001,
  "optimizer": "AdamW",
  "warmup_epochs": 50,
  "activation": "Sigmoid",
  "loss_fn": "DiceCELoss + LAMBDA*MSELoss",
  "dropout": 0,
  "num_epochs": 200,
  "batch_size": 8,
  "lambda": 1,
  "lr_scheduler": "LinearWarmupCosineAnnealingLR",
  "augmentation": "transform"
}
wandb: {
  "project": "hyperparameter-tuning",
  "config": {
        "DATASET": "NerveDataset",
        "MODEL": "JointLearningModel",
        "LEARNING_RATE": 0.0001,
        "WARMUP_EPOCHS": 50,
        "OPTIMIZER": "AdamW",
        "BATCH_SIZE": 8,
        "LOSS_FN": "DiceCELoss + LAMBDA*MSELoss",
        "IMG_SIZE": "(320, 336)",
        "IN_CHANNELS": 1,
        "PATCH_SIZE": 16,
        "DECODER_DIM": 768,
        "MASKING_RATIO": 0.75,
        "OUT_CHANNELS": 1,
        "LAMBDA": 1,
        "NUM_EPOCHS": 200,
        "SCHEDULER": "LinearWarmupCosineAnnealingLR"
        },
  "run_name": "BaselineModel-200epochs"
}
